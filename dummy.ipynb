{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file=pd.read_parquet(r'C:\\Users\\admin\\Desktop\\MapUp-Data-Assessment-E\\evaluation_data\\input\\raw_data.parquet')\n",
    "print(file.head())\n",
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       latitude  longitude                 timestamp  unit\n",
      "0      40.23426  -77.13796 2023-12-05 23:13:40+00:00  1000\n",
      "1      40.23422  -77.13796 2023-12-05 23:13:40+00:00  1000\n",
      "2      40.23419  -77.13795 2023-12-05 23:13:40+00:00  1000\n",
      "3      40.23412  -77.13792 2023-12-05 23:13:40+00:00  1000\n",
      "4      40.23412  -77.13792 2023-12-05 23:13:40+00:00  1000\n",
      "46500  31.13662  -81.58010 2023-12-11 21:58:42+00:00  2000\n",
      "46501  31.13678  -81.58042 2023-12-11 21:58:44+00:00  2000\n",
      "46502  31.13708  -81.58125 2023-12-11 21:58:49+00:00  2000\n",
      "46503  31.13716  -81.58146 2023-12-11 21:58:50+00:00  2000\n",
      "46504  31.13716  -81.58146 2023-12-11 21:58:50+00:00  2000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file=pd.read_parquet(r'C:\\Users\\admin\\Desktop\\MapUp-Data-Assessment-E\\evaluation_data\\input\\raw_data.parquet' ,engine='pyarrow')\n",
    "\n",
    "file['timestamp']= pd.to_datetime(file['timestamp'])\n",
    "\n",
    "df=file\n",
    "\n",
    "#df[\"timestamp\"].diff()\n",
    "a=(df.groupby('unit'))\n",
    "print(a.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process1\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def process_gps_data(parquet_file):\n",
    "    # Read Parquet file into a DataFrame\n",
    "    df = pd.read_parquet(parquet_file,engine='pyarrow')\n",
    "    df['timestamp']= pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    # Sort the DataFrame by unit and timestamp\n",
    "    df.sort_values(by=['unit', 'timestamp'], inplace=True)\n",
    "\n",
    "    # Create a new column 'trip_number' to identify trips\n",
    "    df['trip_number'] = (df.groupby('unit')['timestamp']\n",
    "                         .diff()\n",
    "                         .gt(pd.Timedelta(hours=7))\n",
    "                         .cumsum()\n",
    "                         .fillna(0)\n",
    "                         .astype(int))\n",
    "\n",
    "    # Iterate over unique units and write trip-specific CSV files\n",
    "    for unit, unit_df in df.groupby('unit'):\n",
    "        for trip_number, trip_df in unit_df.groupby('trip_number'):\n",
    "            # Construct CSV file name\n",
    "            csv_filename = f'{unit}_{trip_number}.csv'\n",
    "\n",
    "            # Write trip-specific CSV file\n",
    "            trip_df[['latitude', 'longitude', 'timestamp']].to_csv(csv_filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'your_parquet_file.parquet' with the actual file path\n",
    "    process_gps_data(r'C:\\Users\\admin\\Desktop\\MapUp-Data-Assessment-E\\evaluation_data\\input\\raw_data.parquet' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process1\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def process_gps_data(parquet_file):\n",
    "    # Read Parquet file into a DataFrame\n",
    "    df = pd.read_parquet(parquet_file,engine='pyarrow')\n",
    "    df['timestamp']= pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    # Sort the DataFrame by unit and timestamp\n",
    "    df.sort_values(by=['unit', 'timestamp'], inplace=True)\n",
    "\n",
    "    # Create a new column 'trip_number' to identify trips\n",
    "    df['trip_number'] = (df.groupby('unit')['timestamp']\n",
    "                         .diff()\n",
    "                         .gt(pd.Timedelta(hours=7))\n",
    "                         .cumsum()\n",
    "                         .fillna(0)\n",
    "                         .astype(int))\n",
    "\n",
    "    # Iterate over unique units and write trip-specific CSV files\n",
    "    for unit, unit_df in df.groupby('unit'):\n",
    "        for trip_number, trip_df in unit_df.groupby('trip_number'):\n",
    "            # Construct CSV file name\n",
    "            csv_filename = f'{unit}_{trip_number}.csv'\n",
    "\n",
    "            # Write trip-specific CSV file\n",
    "            trip_df[['latitude', 'longitude', 'timestamp']].to_csv(csv_filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'your_parquet_file.parquet' with the actual file path\n",
    "    process_gps_data(r'C:\\Users\\admin\\Desktop\\MapUp-Data-Assessment-E\\evaluation_data\\input\\raw_data.parquet' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] /output/process1 /output\n",
      "ipykernel_launcher.py: error: the following arguments are required: /output/process1, /output\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#Process 2\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import argparse\n",
    "import concurrent.futures\n",
    "import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv.load_dotenv()\n",
    "print(os.getenv(\"TOLLGURU_API_KEY\"))\n",
    "\n",
    "def upload_to_tollguru(csv_file, output_dir):\n",
    "    # API endpoint and parameters\n",
    "    url = f'{os.getenv(\"TOLLGURU_API_URL\")}/gps-tracks-csv-upload'\n",
    "    params = {'mapProvider': 'osrm', 'vehicleType': '5AxlesTruck'}\n",
    "\n",
    "    # Prepare headers with API key\n",
    "    headers = {'x-api-key': os.getenv(\"TOLLGURU_API_KEY\"), 'Content-Type': 'text/csv'}\n",
    "\n",
    "    # Read CSV file\n",
    "    with open(csv_file, 'rb') as file:\n",
    "        # Send request to TollGuru API\n",
    "        response = requests.post(url, params=params, data=file, headers=headers)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Construct JSON file name\n",
    "            json_filename = os.path.join(output_dir, os.path.splitext(os.path.basename(csv_file))[0] + '.json')\n",
    "\n",
    "            # Save JSON response to file\n",
    "            with open(json_filename, 'w') as json_file:\n",
    "                json_file.write(response.text)\n",
    "\n",
    "            print(f'Successfully processed {csv_file} and saved JSON response to {json_filename}')\n",
    "        else:\n",
    "            print(f'Error processing {csv_file}. Status code: {response.status_code}, Message: {response.text}')\n",
    "\n",
    "def process_gps_files(to_process, output_dir):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # List all CSV files in the input directory\n",
    "    csv_files = [os.path.join(to_process, file) for file in os.listdir(to_process) if file.endswith('.csv')]\n",
    "\n",
    "    # Use concurrent futures to upload files concurrently\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        executor.map(upload_to_tollguru, csv_files, [output_dir] * len(csv_files))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up command-line argument parser\n",
    "    parser = argparse.ArgumentParser(description='Upload GPS tracks to TollGuru API')\n",
    "    parser.add_argument(r'/output/process1')\n",
    "    parser.add_argument(r'/output')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Upload GPS tracks to TollGuru API\n",
    "    process_gps_files(r'C:\\Users\\admin\\Desktop\\MapUp-Data-Assessment-E\\sample_data\\output\\process1', r'C:\\Users\\admin\\Desktop\\MapUp-Data-Assessment-E\\sample_data\\output\\process2')\n",
    "    print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
